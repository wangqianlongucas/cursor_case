#!/usr/bin/env python3
"""
Demo script showing how to use the unified TSP environment with different algorithms
"""

import numpy as np
import matplotlib.pyplot as plt
from unified_env import UnifiedTSPEnvironment

def demo_environment_modes():
    """Demonstrate the difference between simple and enhanced modes."""
    print("ğŸš€ TSPå¼ºåŒ–å­¦ä¹ é¡¹ç›®æ¼”ç¤º")
    print("="*60)
    
    # Create environments
    print("\n1. åˆ›å»ºç»Ÿä¸€ç¯å¢ƒ")
    env_simple = UnifiedTSPEnvironment(num_cities=5, seed=42, state_mode='simple')
    env_enhanced = UnifiedTSPEnvironment(num_cities=5, seed=42, state_mode='enhanced')
    
    print(f"   Simpleæ¨¡å¼çŠ¶æ€ç»´åº¦: {env_simple.get_state_size()}")
    print(f"   Enhancedæ¨¡å¼çŠ¶æ€ç»´åº¦: {env_enhanced.get_state_size()}")
    
    # Show state representations
    print("\n2. çŠ¶æ€è¡¨ç¤ºå¯¹æ¯”")
    state_simple = env_simple.reset()
    state_enhanced = env_enhanced.reset()
    
    print(f"   SimpleçŠ¶æ€: {state_simple}")
    print(f"   EnhancedçŠ¶æ€å½¢çŠ¶: {state_enhanced.shape}")
    print(f"   EnhancedçŠ¶æ€ç±»å‹: {type(state_enhanced)}")
    
    # Show action spaces
    print("\n3. åŠ¨ä½œç©ºé—´")
    valid_actions = env_simple.get_valid_actions()
    action_mask = env_enhanced.get_action_mask()
    
    print(f"   æœ‰æ•ˆåŠ¨ä½œ: {valid_actions}")
    print(f"   åŠ¨ä½œæ©ç : {action_mask}")
    
    return env_simple, env_enhanced

def demo_random_agent(env, num_episodes=5):
    """Demonstrate a random agent playing TSP."""
    print(f"\n4. éšæœºæ™ºèƒ½ä½“æ¼”ç¤º ({env.state_mode}æ¨¡å¼)")
    print("-" * 40)
    
    distances = []
    
    for episode in range(num_episodes):
        state = env.reset()
        total_distance = 0
        done = False
        steps = 0
        
        print(f"   Episode {episode + 1}: ", end="")
        
        while not done and steps < env.num_cities:
            valid_actions = env.get_valid_actions()
            if not valid_actions:
                break
                
            action = np.random.choice(valid_actions)
            state, reward, done, info = env.step(action)
            
            if info["valid"]:
                steps += 1
                print(f"{action}â†’", end="")
            else:
                break
        
        if len(env.visited_cities) == env.num_cities:
            distances.append(env.total_distance)
            print(f"0 (è·ç¦»: {env.total_distance:.1f})")
        else:
            print("æœªå®Œæˆ")
    
    if distances:
        print(f"   å¹³å‡è·ç¦»: {np.mean(distances):.1f}")
        print(f"   æœ€ä½³è·ç¦»: {np.min(distances):.1f}")
    
    return distances

def demo_visualization(env):
    """Demonstrate environment visualization."""
    print(f"\n5. å¯è§†åŒ–æ¼”ç¤º")
    print("-" * 40)
    
    # Reset and take a few random steps
    env.reset()
    
    # Take some random actions to create a partial path
    for _ in range(3):
        valid_actions = env.get_valid_actions()
        if valid_actions:
            action = np.random.choice(valid_actions)
            env.step(action)
    
    print(f"   å½“å‰è·¯å¾„: {env.visited_cities}")
    print(f"   å½“å‰è·ç¦»: {env.total_distance:.1f}")
    print(f"   å‰©ä½™åŸå¸‚: {env.get_valid_actions()}")
    
    # Show the visualization
    env.render(title_suffix="Demo")

def demo_algorithm_compatibility():
    """Demonstrate how different algorithms would use the environment."""
    print(f"\n6. ç®—æ³•å…¼å®¹æ€§æ¼”ç¤º")
    print("-" * 40)
    
    # Q-learning style usage
    print("   Q-learningä½¿ç”¨æ–¹å¼:")
    env_q = UnifiedTSPEnvironment(num_cities=5, seed=42, state_mode='simple')
    state = env_q.reset()
    print(f"     çŠ¶æ€: {state}")
    print(f"     çŠ¶æ€é”®: {(tuple(state), env_q.current_city)}")  # Q-table key
    
    # DQN style usage  
    print("\n   DQNä½¿ç”¨æ–¹å¼:")
    env_dqn = UnifiedTSPEnvironment(num_cities=5, seed=42, state_mode='enhanced')
    state = env_dqn.reset()
    print(f"     çŠ¶æ€å½¢çŠ¶: {state.shape}")
    print(f"     çŠ¶æ€èŒƒå›´: [{state.min():.2f}, {state.max():.2f}]")
    
    # PPO style usage
    print("\n   PPOä½¿ç”¨æ–¹å¼:")
    env_ppo = UnifiedTSPEnvironment(num_cities=5, seed=42, state_mode='enhanced')
    state = env_ppo.reset()
    action_mask = env_ppo.get_action_mask()
    print(f"     çŠ¶æ€å½¢çŠ¶: {state.shape}")
    print(f"     åŠ¨ä½œæ©ç : {action_mask}")
    print(f"     æ©ç åŠ¨ä½œæ•°: {action_mask.sum()}")

def demo_performance_comparison():
    """Demonstrate performance comparison setup."""
    print(f"\n7. æ€§èƒ½å¯¹æ¯”è®¾ç½®")
    print("-" * 40)
    
    # Show how to set up fair comparison
    seed = 42
    num_cities = 8
    
    print(f"   é—®é¢˜è®¾ç½®: {num_cities}åŸå¸‚, ç§å­={seed}")
    
    # Create environments for each algorithm
    env_q = UnifiedTSPEnvironment(num_cities=num_cities, seed=seed, state_mode='simple')
    env_dqn = UnifiedTSPEnvironment(num_cities=num_cities, seed=seed, state_mode='enhanced')
    env_ppo = UnifiedTSPEnvironment(num_cities=num_cities, seed=seed, state_mode='enhanced')
    
    # Verify they have the same problem instance
    cities_match_dqn = np.allclose(env_q.cities, env_dqn.cities)
    cities_match_ppo = np.allclose(env_q.cities, env_ppo.cities)
    
    print(f"   åŸå¸‚ä½ç½®ä¸€è‡´æ€§: Q-learning vs DQN: {cities_match_dqn}")
    print(f"   åŸå¸‚ä½ç½®ä¸€è‡´æ€§: Q-learning vs PPO: {cities_match_ppo}")
    
    # Show state size differences
    print(f"   çŠ¶æ€ç»´åº¦å¯¹æ¯”:")
    print(f"     Q-learning: {env_q.get_state_size()}")
    print(f"     DQN: {env_dqn.get_state_size()}")
    print(f"     PPO: {env_ppo.get_state_size()}")

def main():
    """Run the complete demo."""
    # Demo 1: Environment modes
    env_simple, env_enhanced = demo_environment_modes()
    
    # Demo 2: Random agents
    print("\n" + "="*60)
    distances_simple = demo_random_agent(env_simple, num_episodes=3)
    distances_enhanced = demo_random_agent(env_enhanced, num_episodes=3)
    
    # Demo 3: Visualization
    print("\n" + "="*60)
    demo_visualization(env_enhanced)
    
    # Demo 4: Algorithm compatibility
    print("\n" + "="*60)
    demo_algorithm_compatibility()
    
    # Demo 5: Performance comparison setup
    print("\n" + "="*60)
    demo_performance_comparison()
    
    # Summary
    print("\n" + "="*60)
    print("ğŸ‰ æ¼”ç¤ºå®Œæˆ!")
    print("\nğŸ“š æ¥ä¸‹æ¥å¯ä»¥å°è¯•:")
    print("   â€¢ python src/train.py           - Q-learningè®­ç»ƒ")
    print("   â€¢ python src/dqn_train.py       - DQNè®­ç»ƒ")
    print("   â€¢ python src/ppo_train.py       - PPOè®­ç»ƒ")
    print("   â€¢ python src/compare_all_methods.py - å…¨é¢å¯¹æ¯”")
    print("\nğŸ’¡ æç¤º: æŸ¥çœ‹SETUP_GUIDE.mdäº†è§£è¯¦ç»†é…ç½®è¯´æ˜")

if __name__ == "__main__":
    main() 